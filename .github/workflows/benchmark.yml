name: Performance Benchmarks

on:
  push:
    branches: [master, main, develop]
    paths:
      - 'sdk_agent/**'
      - 'tests/benchmarks/**'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [master, main, develop]
    paths:
      - 'sdk_agent/**'
      - 'tests/benchmarks/**'
  workflow_dispatch:  # Allow manual trigger

jobs:
  benchmark:
    name: Run Performance Benchmarks
    runs-on: ubuntu-latest

    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comparing with baseline

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Cache benchmark baselines
        uses: actions/cache@v4
        with:
          path: .benchmarks
          key: benchmarks-py${{ matrix.python-version }}-${{ github.ref_name }}-${{ github.sha }}
          restore-keys: |
            benchmarks-py${{ matrix.python-version }}-${{ github.ref_name }}-
            benchmarks-py${{ matrix.python-version }}-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark pytest-asyncio

      - name: Run benchmarks
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-autosave \
            --benchmark-save=current_run \
            --benchmark-json=benchmark_results.json \
            -v

      - name: Store benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-py${{ matrix.python-version }}
          path: |
            benchmark_results.json
            .benchmarks/
          retention-days: 30

      - name: Compare with baseline (on main branch)
        if: github.ref == 'refs/heads/master' || github.ref == 'refs/heads/main'
        continue-on-error: true
        run: |
          if [ -d ".benchmarks" ]; then
            pytest tests/benchmarks/ \
              --benchmark-only \
              --benchmark-compare=0001 \
              --benchmark-compare-fail=mean:10% \
              -v || echo "Performance regression detected (>10% slower)"
          else
            echo "No baseline found, skipping comparison"
          fi

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            try {
              const results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
              const benchmarks = results.benchmarks;

              let comment = '## ðŸ“Š Performance Benchmark Results\n\n';
              comment += `**Python Version:** ${{ matrix.python-version }}\n\n`;
              comment += '| Test | Mean | Std Dev | Min | Max |\n';
              comment += '|------|------|---------|-----|-----|\n';

              benchmarks.forEach(bench => {
                const stats = bench.stats;
                comment += `| ${bench.name} | ${stats.mean.toFixed(4)}s | ${stats.stddev.toFixed(4)}s | ${stats.min.toFixed(4)}s | ${stats.max.toFixed(4)}s |\n`;
              });

              comment += '\n### Performance Targets\n\n';
              comment += '- âœ… 50 files: < 1s (target), < 2s (acceptable)\n';
              comment += '- âœ… 100 files: < 2s (target), < 4s (acceptable)\n';
              comment += '- âœ… Throughput: > 100 files/sec (target), > 50 files/sec (acceptable)\n';

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not post benchmark results:', error.message);
            }

  benchmark-summary:
    name: Benchmark Summary
    runs-on: ubuntu-latest
    needs: benchmark
    if: always()

    steps:
      - name: Download all benchmark artifacts
        uses: actions/download-artifact@v4
        with:
          path: benchmark-results

      - name: Generate summary
        run: |
          echo "## ðŸŽ¯ Benchmark Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark results from all Python versions:" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          for dir in benchmark-results/*/; do
            if [ -f "$dir/benchmark_results.json" ]; then
              py_version=$(basename "$dir" | sed 's/benchmark-results-py//')
              echo "### Python $py_version" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
              echo "âœ… Benchmarks completed successfully" >> $GITHUB_STEP_SUMMARY
              echo "" >> $GITHUB_STEP_SUMMARY
            fi
          done

          echo "### Performance Guidelines" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Target | Acceptable | Warning |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|--------|------------|---------|" >> $GITHUB_STEP_SUMMARY
          echo "| **50 files** | < 1s | < 2s | > 2s |" >> $GITHUB_STEP_SUMMARY
          echo "| **100 files** | < 2s | < 4s | > 4s |" >> $GITHUB_STEP_SUMMARY
          echo "| **Throughput** | 100 files/s | 50 files/s | < 50 files/s |" >> $GITHUB_STEP_SUMMARY
          echo "| **Memory (1K files)** | < 50MB | < 100MB | > 100MB |" >> $GITHUB_STEP_SUMMARY
          echo "| **P95 Latency** | < 3ms | < 5ms | > 5ms |" >> $GITHUB_STEP_SUMMARY

  detect-regression:
    name: Detect Performance Regression
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4

      - name: Checkout base branch
        uses: actions/checkout@v4
        with:
          ref: ${{ github.base_ref }}
          path: base

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest-benchmark pytest-asyncio

      - name: Run baseline benchmarks
        working-directory: ./base
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-save=baseline \
            --benchmark-json=baseline_results.json \
            -v

      - name: Run PR benchmarks
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-save=pr_run \
            --benchmark-json=pr_results.json \
            -v

      - name: Compare performance
        id: compare
        run: |
          pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-compare=0001 \
            --benchmark-compare-fail=mean:10% \
            -v || echo "regression=true" >> $GITHUB_OUTPUT

      - name: Comment on regression
        if: steps.compare.outputs.regression == 'true'
        uses: actions/github-script@v7
        with:
          script: |
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: 'âš ï¸ **Performance Regression Detected**\n\n' +
                    'This PR shows performance regression > 10% compared to the base branch.\n\n' +
                    'Please review the benchmark results and optimize if necessary.\n\n' +
                    '### Next Steps\n' +
                    '1. Review the benchmark comparison results\n' +
                    '2. Identify the slow operations\n' +
                    '3. Profile the code with `cProfile` or `py-spy`\n' +
                    '4. Consider optimization techniques:\n' +
                    '   - Increase concurrency\n' +
                    '   - Optimize batch sizes\n' +
                    '   - Reduce I/O operations\n' +
                    '   - Cache frequently accessed data'
            });

      - name: Fail on significant regression
        if: steps.compare.outputs.regression == 'true'
        run: |
          echo "::error::Performance regression detected! Mean execution time increased by more than 10%"
          exit 1
